{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "- 파서(html,xml 의 형태로 내려온 데이터를  요소만 찾기 위해 필요)\n",
    "\n",
    "\n",
    "- requests + BeautifulSoup : 이 조합으로 주로 크롤링을 함\n",
    "\n",
    "- 파서 종류 \n",
    "    - html.parser (두번쨰 정도)\n",
    "    - lxml (속도가 가장 빠름) 설치가 필요 pip install lxml\n",
    "    - html5lib (가장 느림)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head data-cloud-area=\"head\">\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "<meta content=\"width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no\" name=\"viewport\"/>\n",
      "<meta content=\"telephone=no\" name=\"format-detection\"/>\n",
      "<meta content=\"unsafe-url\" name=\"referrer\"/>\n",
      "<title>“일자리에서 시작하는 희망의 날개짓.. 공항에서 펼쳐볼까?”</title>\n",
      "<link href=\"//t1.daumcdn.net/top/favicon.ico\" rel=\"shortcut icon\"/>\n",
      "<meta content=\"8WDvU7Ru4B\" property=\"mccp:docId\"/>\n",
      "<meta content=\"언론사 뷰\" property=\"og:site_name\"/>\n",
      "<meta content=\"“일자리에서 시작하는 희망의 날개짓.. 공항에서 펼쳐볼까?”\" property=\"og:title\"/>\n",
      "<meta content=\"20240524094116\" property=\"og:regDate\"/>\n",
      "<meta content=\"article\" property=\"og:type\"/>\n",
      "<meta content=\"JIBS\" property=\"og:article:author\"/>\n",
      "<meta content=\"https://v.daum.net/v/20240524094116167\" property=\"og:url\"/>\n",
      "<meta content=\"https://img1.daumcdn.net/thumb/S1200x630/?fname=https://t1.daumcdn.net/news/202405/24/jibs/20240524094117272ocwc.jpg\" property=\"og:image\"/>\n",
      "<meta content=\"1200\" property=\"og:image:width\"/>\n",
      "<meta content=\"630\" property=\"og:image:height\"/>\n",
      "<meta content=\"# 해외여행 수요가 회복되면서, 성수기를 대비한 국내 항공사들의 인력 채용 움직임이 활발한 모습입니다. 코로나 19로 인해 중단됐던 노선과 항공편을 증설하고 일본·동남아 등을 중심으로 국외 여객 수요가 지속 늘어나면서 항공사마다 새로운 기재 도입과 노선 증편 등 대응을 서두르고 있습니다. 인력 확보엔 공항도 빠질 수 없습니다. 급증하는 항공기와 여객 증\" property=\"og:description\"/>\n",
      "<meta content=\"daumapps://web?url=https%3A%2F%2Fv.daum.net/v/20240524094116167\" property=\"al:android:url\"/>\n",
      "<meta content=\"net.daum.android.daum\" property=\"al:android:package\"/>\n",
      "<meta content=\"다음앱\" property=\"al:android:app_name\"/>\n",
      "<meta content=\"daumapps://web?url=https%3A%2F%2Fv.daum.net/v/20240524094116167\" property=\"al:ios:url\"/>\n",
      "<meta content=\"365494029\" property=\"al:ios:app_store_id\"/>\n",
      "<meta content=\"다음앱\" property=\"al:ios:app_name\"/>\n",
      "<meta content=\"summary_large_image\" property=\"twitter:card\"/>\n",
      "<meta content=\"daumapps://web?url=https%3A%2F%2Fv.daum.net/v/20240524094116167\" property=\"twitter:app:url:googleplay\"/>\n",
      "<meta content=\"net.daum.android.daum\" property=\"twitter:app:id:googleplay\"/>\n",
      "<meta content=\"다음앱\" property=\"twitter:app:name:googleplay\"/>\n",
      "<meta content=\"KR\" property=\"twitter:app:country\"/>\n",
      "<meta content=\"daumapps://web?url=https%3A%2F%2Fv.daum.net/v/20240524094116167\" property=\"twitter:app:url:iphone\"/>\n",
      "<meta content=\"365494029\" property=\"twitter:app:id:iphone\"/>\n",
      "<meta content=\"다음앱\" property=\"twitter:app:name:iphone\"/>\n",
      "<meta content=\"daumapps://web?url=https%3A%2F%2Fv.daum.net/v/20240524094116167\" property=\"twitter:app:url:ipad\"/>\n",
      "<meta content=\"365494029\" property=\"twitter:app:id:ipad\"/>\n",
      "<meta content=\"다음앱\" property=\"twitter:app:name:ipad\"/>\n",
      "</head>\n",
      "<title>“일자리에서 시작하는 희망의 날개짓.. 공항에서 펼쳐볼까?”</title>\n",
      "<img alt=\"Daum\" class=\"logo_daum\" height=\"18\" src=\"//t1.daumcdn.net/media/common/newsview_2021/pc/rtn/logo_daum.png\" width=\"44\"/>\n",
      "<bound method PageElement.get_text of <title>“일자리에서 시작하는 희망의 날개짓.. 공항에서 펼쳐볼까?”</title>>\n"
     ]
    }
   ],
   "source": [
    "url =\"https://v.daum.net/v/20240524094116167\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r=s.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "    #print(soup)\n",
    "    print(soup.head) # 헤드만 가져와봐\n",
    "    print(soup.title) # 타이틀만 가져와봐\n",
    "    print(soup.img)\n",
    "\n",
    "    # 요소 접근\n",
    "    # 태그명 사용\n",
    "\n",
    "    print(soup.title.get_text)# 타이틀 태그의 내용을 가지고 오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“일자리에서 시작하는 희망의 날개짓.. 공항에서 펼쳐볼까?”\n",
      "{'class': ['tit_view'], 'data-translation': 'true'}\n"
     ]
    }
   ],
   "source": [
    "url =\"https://v.daum.net/v/20240524094116167\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r=s.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "\n",
    "\n",
    "    # 요소 접근\n",
    "    # 태그명 사용\n",
    "\n",
    "\n",
    "    print(soup.title.get_text())# 타이틀 태그의 내용을 가지고 오기\n",
    "    # attrs 태그 속성 추출\n",
    "    print(soup.h3.attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title<title>The Dormouse's story</title>\n",
      "title content The Dormouse's story\n",
      "title content The Dormouse's story\n",
      "title parent <head>\n",
      "<title>The Dormouse's story</title>\n",
      "</head>\n",
      "title parent <head>\n",
      "<title>The Dormouse's story</title>\n",
      "</head>\n",
      "===========================================\n",
      "===========================================\n",
      "===========================================\n",
      "p <p class=\"title\">\n",
      "<b> The Dormouse's story </b>\n",
      "</p>\n",
      "p The Dormouse's story\n",
      "p {'class': ['title']}\n",
      "p ['title']\n",
      "b The Dormouse's story\n"
     ]
    }
   ],
   "source": [
    "url =\"./story.html\"\n",
    "\n",
    "with open(url,\"r\")as f:\n",
    "    r=f.read()\n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "     # print(soup)\n",
    "\n",
    "\n",
    "    # 타이틀 태그 가져오게\n",
    "\n",
    "    title=soup.title\n",
    "\n",
    "    print(f\"title{title}\")\n",
    "    print(f\"title content {title.get_text()}\") # get_text == string\n",
    "    print(f\"title content {title.string}\") # string ==  get_text 둘다 똑같은 역할을 하고 안되면 둘중 하나로 하자\n",
    "    print(f\"title parent {title.parent}\")\n",
    "    print(f\"title parent {title.parent}\")\n",
    "\n",
    "    print(\"===========================================\")\n",
    "    print(\"===========================================\")\n",
    "    print(\"===========================================\")\n",
    "\n",
    "    # p 태그 가져오기\n",
    "\n",
    "    # 파싱을 한다고해서 공백을 빼주진 않기에  strip() 으로 공백을 제거 해주고 가져와준다 보통 이렇게 한다\n",
    "\n",
    "    p1 = soup.p\n",
    "\n",
    "    print(f\"p {p1}\")\n",
    "    print(f\"p {p1.get_text().strip()}\")\n",
    "    print(f\"p {p1.attrs}\")\n",
    "    print(f\"p {p1['class']}\") # 딕셔너리 값이 존재하기에 키 값으로 접근 가능\n",
    "\n",
    "    b = soup.b\n",
    "    print(f\"b {b.get_text().strip()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2 <p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>\n",
      "p2 Once upon a time there were three little sisters; and their names were\n",
      "       Elsie \n",
      "      ,\n",
      "       Lacie \n",
      "      and\n",
      "       Tillie \n",
      "      ; and they lived at the bottom of a well.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp2\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mp2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp2\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 딕셔너리 값이 존재하기에 키 값으로 접근 가능\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 문서의 구조를 이용한 요소 찾기\n",
    "# parent ,children, next_sibling ....\n",
    "\n",
    "url =\"./story.html\"\n",
    "\n",
    "with open(url,\"r\")as f:\n",
    "    r=f.read()\n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "\n",
    "    # body = soup.body\n",
    "    #  # 바디의 자식들을 가져와라\n",
    "    # print(f\"body chaidren {body.children}\")\n",
    "\n",
    "    # for child in body:\n",
    "    #     print(child)\n",
    "\n",
    "    # 첫번쨰 p 요소 찾기\n",
    "    p1 = soup.p\n",
    "    # p1 의 다음 형제 찾아와봐\n",
    "\n",
    "# 문서의 구조를 이용한 요소 찾기\n",
    "# parent ,children, next_sibling ....\n",
    "\n",
    "url =\"./story.html\"\n",
    "\n",
    "with open(url,\"r\")as f:\n",
    "    r=f.read()\n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "\n",
    "    # body = soup.body\n",
    "    #  # 바디의 자식들을 가져와라\n",
    "    # print(f\"body chaidren {body.children}\")\n",
    "\n",
    "    # for child in body:\n",
    "    #     print(child)\n",
    "\n",
    "    # 첫번쨰 p 요소 찾기\n",
    "    p1 = soup.p\n",
    "    # p1 의 다음 형제 찾아와봐\n",
    "    p2=p1.find_next_sibling(\"p\")\n",
    "\n",
    "    print(f\"p2 {p2}\")\n",
    "    print(f\"p2 {p2.get_text().strip()}\")\n",
    "    # print(f\"p2 {p2.string.strip()}\")\n",
    "    print(f\"p2 {p2.attrs}\")\n",
    "    print(f\"p2 {p2['class']}\") # 딕셔너리 값이 존재하기에 키 값으로 접근 가능\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "    print(f\"p2 {p2}\")\n",
    "    print(f\"p2 {p2.get_text().strip()}\")\n",
    "    # print(f\"p2 {p2.string.strip()}\")\n",
    "    print(f\"p2 {p2.attrs}\")\n",
    "    print(f\"p2 {p2['class']}\") # 딕셔너리 값이 존재하기에 키 값으로 접근 가능\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      "<title>The Dormouse's story</title>\n",
      "</head>\n",
      "<p class=\"title\">\n",
      "<b> The Dormouse's story </b>\n",
      "</p>\n",
      "<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>\n",
      "[<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>, <p class=\"story\">...</p>]\n",
      "======================\n",
      "======================\n",
      "======================\n",
      "<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>\n",
      "<p class=\"story\">...</p>\n",
      "==============================\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "http://example.com/tillie\n",
      "=====================================\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n"
     ]
    }
   ],
   "source": [
    "# find() :  조건을 만족하느 요소 하나 찾기\n",
    "# find_all() 조건을 만족하는 요소 전부 찾기\n",
    "\n",
    "\n",
    "\n",
    "url =\"./story.html\"\n",
    "\n",
    "with open(url,\"r\")as f:\n",
    "    r=f.read()\n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "    head=soup.find(\"head\")\n",
    "    print(head)\n",
    "\n",
    "    # p1 = soup.find(\"p\")\n",
    "    # print(p1)\n",
    "    # 클래스가 타이틀인거 찾아와봐    \n",
    "\n",
    "\n",
    "    p1 = soup.find(\"p\",attrs={\"class\":\"title\"})\n",
    "    print(p1)\n",
    "\n",
    "        \n",
    "    # p2 = soup.find(\"p\",attrs={\"class\":\"story\"})\n",
    "\n",
    "    # 특정 클래스 속성을 가진 p 요소 찾기\n",
    "    p2=soup.find(\"p\",class_=\"story\")\n",
    "    print(p2)\n",
    "\n",
    "    p_all=soup.find_all(\"p\", class_=\"story\")\n",
    "    print(p_all)\n",
    "\n",
    "    print(\"======================\")\n",
    "    print(\"======================\")\n",
    "    print(\"======================\")\n",
    "\n",
    "    for p_all_row in p_all:\n",
    "        print(p_all_row)\n",
    "\n",
    "\n",
    "  \n",
    "        pass\n",
    "print(\"=\"*30)\n",
    "\n",
    "a1 = soup.find(\"a\",id = \"link1\")\n",
    "print(a1)\n",
    "\n",
    "a3 = soup.find(\"a\",id = \"link3\")\n",
    "# 경로만 따기\n",
    "print(a3[\"href\"])\n",
    "\n",
    "\n",
    "print(\"=====================================\")\n",
    "a_tags=soup.find_all(\"a\", limit=2) # 이렇게 리미트를 둬서 찾아올 갯수를 제한을 둘 수 있다\n",
    "\n",
    "for ele in a_tags:\n",
    "    print(ele[\"href\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elsie', 'Lacie', 'Tillie']\n",
      "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url =\"./story.html\"\n",
    "\n",
    "with open(url,\"r\")as f:\n",
    "    r=f.read()\n",
    "\n",
    "    soup = BeautifulSoup(r,\"lxml\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "    # link1=soup.find_all(string=\"Elsie\") # 드는 BeautifulSoup을 사용하여 HTML 문서에서 텍스트가 \"Elsie\"인 모든 요소를 찾는 방법입니다. 이 메서드는 요소의 태그나 속성 대신 텍스트 내용을 기준으로 요소를 찾습니다.\n",
    "    link1=soup.find_all(string=[\"Elsie\",\"Lacie\",\"Tillie\"])\n",
    "    link2=soup.find_all(\"a\",string=[\"elsie\",\"Lacie\",\"Tillie\"])\n",
    "\n",
    "    print(link1)\n",
    "    print(link2)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n",
      "Well, Prince, so Genoa and Lucca are now just family estates of the\n",
      "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
      "if you still try to defend the infamies and horrors perpetrated by\n",
      "that Antichrist- I really believe he is Antichrist- I will have\n",
      "nothing more to do with you and you are no longer my friend, no longer\n",
      "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
      "I have frightened you- sit down and tell me all the news.\n",
      "If you have nothing better to do, Count [or Prince], and if the\n",
      "prospect of spending an evening with a poor invalid is not too\n",
      "terrible, I shall be very charmed to see you tonight between 7 and 10-\n",
      "Annette Scherer.\n",
      "Heavens! what a virulent attack!\n",
      "First of all, dear friend, tell me how you are. Set your friend's\n",
      "mind at rest,\n",
      "Can one be well while suffering morally? Can one be calm in times\n",
      "like these if one has any feeling?\n",
      "You are\n",
      "staying the whole evening, I hope?\n",
      "And the fete at the English ambassador's? Today is Wednesday. I\n",
      "must put in an appearance there,\n",
      "My daughter is\n",
      "coming for me to take me there.\n",
      "I thought today's fete had been canceled. I confess all these\n",
      "festivities and fireworks are becoming wearisome.\n",
      "If they had known that you wished it, the entertainment would\n",
      "have been put off,\n",
      "Don't tease! Well, and what has been decided about Novosiltsev's\n",
      "dispatch? You know everything.\n",
      "What can one say about it?\n",
      "What has been decided? They have decided that\n",
      "Buonaparte has burnt his boats, and I believe that we are ready to\n",
      "burn ours.\n",
      "Oh, don't speak to me of Austria. Perhaps I don't understand\n",
      "things, but Austria never has wished, and does not wish, for war.\n",
      "She is betraying us! Russia alone must save Europe. Our gracious\n",
      "sovereign recognizes his high vocation and will be true to it. That is\n",
      "the one thing I have faith in! Our good and wonderful sovereign has to\n",
      "perform the noblest role on earth, and he is so virtuous and noble\n",
      "that God will not forsake him. He will fulfill his vocation and\n",
      "crush the hydra of revolution, which has become more terrible than\n",
      "ever in the person of this murderer and villain! We alone must\n",
      "avenge the blood of the just one.... Whom, I ask you, can we rely\n",
      "on?... England with her commercial spirit will not and cannot\n",
      "understand the Emperor Alexander's loftiness of soul. She has\n",
      "refused to evacuate Malta. She wanted to find, and still seeks, some\n",
      "secret motive in our actions. What answer did Novosiltsev get? None.\n",
      "The English have not understood and cannot understand the\n",
      "self-abnegation of our Emperor who wants nothing for himself, but only\n",
      "desires the good of mankind. And what have they promised? Nothing! And\n",
      "what little they have promised they will not perform! Prussia has\n",
      "always declared that Buonaparte is invincible, and that all Europe\n",
      "is powerless before him.... And I don't believe a word that Hardenburg\n",
      "says, or Haugwitz either. This famous Prussian neutrality is just a\n",
      "trap. I have faith only in God and the lofty destiny of our adored\n",
      "monarch. He will save Europe!\n",
      "I think,\n",
      "None\n",
      "In a moment. A propos,\n",
      "None\n",
      "I shall be delighted to meet them,\n",
      "But tell me,\n",
      "is it true that the Dowager Empress wants Baron Funke\n",
      "to be appointed first secretary at Vienna? The baron by all accounts\n",
      "is a poor creature.\n",
      "Baron Funke has been recommended to the Dowager Empress by her\n",
      "sister,\n",
      "Now about your family. Do you know that since your daughter came\n",
      "out everyone has been enraptured by her? They say she is amazingly\n",
      "beautiful.\n",
      "I often think,\n",
      "None\n",
      "Two such charming children. And really you appreciate\n",
      "them less than anyone, and so you don't deserve to have them.\n",
      "I can't help it,\n",
      "Lavater would have said I\n",
      "lack the bump of paternity.\n",
      "Don't joke; I mean to have a serious talk with you. Do you know I\n",
      "am dissatisfied with your younger son? Between ourselves\n",
      "he was mentioned at Her\n",
      "Majesty's and you were pitied....\n",
      "What would you have me do?\n",
      "You know I did all\n",
      "a father could for their education, and they have both turned out\n",
      "fools. Hippolyte is at least a quiet fool, but Anatole is an active\n",
      "one. That is the only difference between them.\n",
      "And why are children born to such men as you? If you were not a\n",
      "father there would be nothing I could reproach you with,\n",
      "I am your faithful slave and to you alone I can confess that my\n",
      "children are the bane of my life. It is the cross I have to bear. That\n",
      "is how I explain it to myself. It can't be helped!\n"
     ]
    }
   ],
   "source": [
    "url=\"https://pythonscraping.com/pages/warandpeace.html\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r=s.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "# 등장 입력 출력\n",
    "\n",
    "#  뭘뤄줄지 몰라서 걍 한글 코딩\n",
    "\n",
    "    등장인물들=soup.find_all(\"span\",class_=\"green\")\n",
    "\n",
    "\n",
    "    for 등장인물 in 등장인물들:\n",
    "        print(등장인물.string) # string 또는 get_text()\n",
    "   \n",
    "\n",
    "\n",
    "# 대사 출력\n",
    "\n",
    "    대사들=soup.find_all(\"span\",class_=\"red\")\n",
    "\n",
    "\n",
    "    for 대사 in 대사들:\n",
    "        print(대사.string) # string 또는 get_text()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넷마블만 보인다\n",
      "이주은\n",
      "2024. 5. 24. 10:11\n",
      "넷마블 게임 5종, 매출 톱20 진입…\"넷마블의 시간 시작됐다\"나혼렙, 일주일 만에 매출 350억원…장기 흥행 기대아스달 연대기·나혼렙·레이븐2·칠대죄 키우기…라인업 '척척'\n",
      "특히 지난 8일 출시한 '나 혼자만 레벨업'은 첫 주에만 무려 350억원의 매출을 올렸는데, 이는 2017년 이후 출시된 모바일게임 중 '오딘: 발할라 라이징'(480억 원)과 '리니지W'(400억 원)에 이은 3위 기록이다. 또 다른 기록도 '나 혼자만 레벨업'의 흥행을 말해주고 있다. '나 혼자만 레벨업'은 출시 첫날 75만명의 일간 사용자 수(DAU)를 기록해 넥슨의 '던전앤파이터 모바일'의 74만명을 앞서기도 했다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url=\"https://v.daum.net/v/20240524101148323\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r=s.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\") # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 뉴스 제목 뽑아오기\n",
    "    뉴스제목=soup.find(\"h3\",class_=\"tit_view\")\n",
    "    print(뉴스제목.string)\n",
    "\n",
    "\n",
    "\n",
    "    # 작성자\n",
    "    작성자=soup.find(\"span\",class_=\"txt_info\")\n",
    "    print(작성자.string)\n",
    "    \n",
    "\n",
    "    # 작성 시간\n",
    "    작성시간=soup.find(\"span\",class_=\"num_date\")\n",
    "    print(작성시간.string)\n",
    "\n",
    "\n",
    "    # 첫번쨰 문단 가져오기\n",
    "    첫번쨰_문단=soup.find(\"strong\",class_=\"summary_view\")\n",
    "    print(첫번쨰_문단.get_text())\n",
    "\n",
    "    # 전체 본문 내용 가져오기\n",
    "\n",
    "    전체본문=soup.find(\"p\",attrs={\"dmcf-pid\":\"uNE22faV2E\"})\n",
    "    print(전체본문.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m r \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(r.text)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m(r, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m title \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp.title > b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(title)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "# css select 사용\n",
    "# select() : 전체 요소 / select_one()\n",
    "\n",
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    title = soup.select_one(\"p.title > b\")\n",
    "    print(title)\n",
    "\n",
    "    link1 = soup.select_one(\"#link1\")\n",
    "    print(link1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m r \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(r.text)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m(r, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m stories \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp.story > a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print(stories)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    stories = soup.select(\"p.story > a\")\n",
    "    # print(stories)\n",
    "\n",
    "    for story in stories:\n",
    "        print(story)\n",
    "        print(story.text)\n",
    "        print(story.string)\n",
    "        print(story.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "==== Elsie\n",
      "==== <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "==== Lacie\n",
      "==== <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "==== Tillie\n",
      "===>  <p class=\"story\">...</p>\n",
      "===>  ...\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url, \"r\") as f:\n",
    "    r = f.read()\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    stories = soup.select(\"p.story\")\n",
    "    # print(stories)\n",
    "\n",
    "    for story in stories:\n",
    "        temp = story.find_all(\"a\")\n",
    "\n",
    "        if temp:\n",
    "            for v in temp:\n",
    "                print(\"====\", v)\n",
    "                print(\"====\", v.string)\n",
    "        else:\n",
    "            print(\"===> \",story)\n",
    "            print(\"===> \",story.string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 : 의대증원 오늘 확정…전공의·의대생 복귀 가능성 더 멀어졌다\n",
      "작성자 : 천선휴 기자\n",
      "작성날짜와 시간 : 2024. 5. 24. 10:48\n",
      "첫번째 문단 (서울=뉴스1) 천선휴 기자 = 2025학년도 의과대학 정원이 24일 확정된다. 1999년 이후 27년 만의 의대정원 증원이다. \n",
      "(서울=뉴스1) 천선휴 기자 = 2025학년도 의과대학 정원이 24일 확정된다. 1999년 이후 27년 만의 의대정원 증원이다. \n",
      "하지만 이로써 학교를 떠나 있는 의대생들과 병원을 이탈한 전공의들이 돌아올 가능성도 더 멀어졌다. \n",
      "한국대학교육협의회는 24일 오후 제2차 대학입학전형위원회를 열고 의대 증원안이 포함된 대입전형 시행계획 변경안을 심의·확정한다. 이에 대한 심의 결과는 30일에, 대학별 모집요강은 31일에 발표된다. 이달 말이면 의대 증원 절차가 모두 마무리되는 것이다.\n",
      "일부 국립대에서는 의대 정원 증원을 확정하기 위해 필요한 학칙 개정이 부결되는 등 학내 갈등이 이어지고 있지만, 그럼에도 2025학년도 대입 전형과 모집정원은 그대로 확정된다는 게 교육부의 설명이다.\n",
      "이로써 내년 전국 40개 의과대학은 전년보다 1509명이 늘어난 4567명의 신입생을 뽑게 된다. 동시에 '원점 재검토'를 복귀의 전제로 주장해오던 전공의와 의대생들이 돌아올 명분도 사라졌다. \n",
      "전공의들이 끝내 돌아오지 않으면 당장 내년에 전문의 2910명이 배출되지 못한다. 이들 중엔 필수의료과에서 수련하던 전공의 1385명도 포함돼 있다. 가뜩이나 부족한 필수과 전문의들마저 1400여 명이 배출되지 않는다는 것이다. \n",
      "전문의 배출이 되지 않을 경우 군의관, 공중보건의 모집에도 영향을 미칠 수밖에 없다.\n",
      "다만 의료계에선 전문의 시험만 남겨둔 레지던트 4년 차(3년제 진료과목은 3년 차) 전공의들은 복귀해 시험을 치를 것이라는 전망도 내놓고 있다. 하지만 대세를 뒤흔들 만큼의 수가 아닌 극소수에 불과할 것으로 보고 있다. \n",
      "의대생들이 돌아올 확률은 더욱 적어 보인다. 전공의들은 가정을 꾸리거나 외벌이인 경우가 많아 생계를 이유로 돌아오길 고민하는 경우도 일부 있다고는 하지만 의대생들의 의지는 확고하다. 현재 의대생 1만 8348명 중 99.7%가 학교에 나오지 않고 있다. \n",
      "이들이 이 상태로 유급에 처해질 경우 현재 1학년인 3058명의 학생들과 내년 신입생 4567명까지 총 7625명이 함께 1학년 수업을 들어야 한다. 각 대학의 현재 시설과 인력으로는 감당할 수 없는 인원이다. \n",
      "또 의대생 졸업자가 나오지 않으면 병원들은 인턴도 뽑지 못하게 된다. \n",
      "문제는 각 대학의 교육 여건이 미진할 경우 한국의학교육평가원이 진행하는 평가에서 인증을 받지 못하게 돼 서남대 의대처럼 폐교를 하게 되는 일이 벌어질 수도 있다는 점이다. \n",
      "한 의과대학 교수는 \"엄청나게 늘어난 저 신입생들을 교육할 여력이 안 된다는 건 너무나 자명한 사실\"이라며 \"2025학년 의예과 1학년인 7625명은 6년간 학교를 같이 다녀야 하는데 향후 6년간 큰 부담을 줄 것\"이라고 말했다. \n",
      "또 이들이 졸업해 인턴을 지원할 때도 문제가 된다. 이 교수는 \"내년부터 늘어난 의대 정원이 졸업하는 2031년 7625명이 인턴을 지원하게 되는 것인데 병원이 얼마나 인턴 선발의 여력이 있을지 모르겠다\"며 \"결국 의대는 졸업하지만 인턴, 레지던트 과정을 밟을 수 없는 의사들은 어떻게 되는 것인가\"라고 우려했다. \n",
      "그러면서 \"대학병원들이 아직도 전공의가 들어오길 바라고, 전공의로 운영되는 진료시스템에 대해 미련을 가지고 있는데 이제 그런 상상은 2036년부터나 가능할 것\"이라고 말했다.\n",
      "정부는 비상진료체계 강화 대책을 잇따라 내놓으며 전공의 이탈에 따른 의료공백을 메우고 있지만 현실적으로 역부족인 상황이다. \n",
      "전공의들이 떠난 지 석달째 비상진료 체계를 운영하면서 공중보건의, 군의관을 파견하고 있지만 547명에 불과해 전공의 1만 3000여 명의 자리를 메우기엔 턱없이 부족하다. \n",
      "또 다른 대책으로 시니어 의사 채용 방안을 내놓고 지난달 16일 시니어 의사 지원센터를 개소했지만 문의 전화는 8건뿐인 데다 예산 확정 등도 되지 않아 초기 세팅 단계에 머무르고 있다.\n",
      "정부가 내세우고 있는 '전문의 중심 병원'도 당장 의료공백을 메울 수 없는 대책이긴 매한가지다. 병원 관계자들은 상급종합병원의 경우 전공의 비율이 40%에 달했는데 이들을 대신해 전문의를 채용하는 것은 불가능하다고 입을 모은다. \n",
      "이 같은 문제를 정부도 모르고 있는 것은 아니다. 조규홍 복지부 장관은 지난 22일 기자간담회에서 \"군의관, 공보의도 투입하고 간호사들의 진료지원도 확대하는 등 대책을 마련하고 있는데 이게 지속가능하진 않다\"며 \"하루빨리 전공의 돌아와서 자리를 메꿔주면 좋겠다\"고 말했다. \n",
      "그러면서 \"의료개혁 특별위원회를 빨리 가동하고 논의를 해서 의료개혁을 완성해가는 것이 정부의 대책\"이라고 덧붙였다. \n",
      "이에 정부는 이날 의개특위 산하 4개 전문위원회 중 의료인력 전문위원회의 첫 회의를 열고 전공의 연속 근무 시간 단축 등을 논의할 계획이다. 이날로 4개 전문위원회의 1차 회의가 마무리되는 것이다. \n",
      "이 전문위원회들은 앞으로 격주 회의를 열고 개혁 과제를 구체화해 나갈 방침이다.\n",
      "sssunhue@news1.kr \n"
     ]
    }
   ],
   "source": [
    "# select(), select_one() 으로 변경\n",
    "\n",
    "url = \"https://v.daum.net/v/20240524104849821\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    # print(r.text)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 뉴스 제목 \n",
    "    title = soup.select_one(\"h3.tit_view\")\n",
    "    print(f\"제목 : {title.text}\")\n",
    "\n",
    "    # 작성자 \n",
    "    writer = soup.select_one(\"span.txt_info\")\n",
    "    print(f\"작성자 : {writer.string}\")\n",
    "\n",
    "    # 작성날짜와 시간\n",
    "    num_date = soup.select_one(\"span.num_date\")\n",
    "    print(f\"작성날짜와 시간 : {num_date.string}\")\n",
    "\n",
    "    # 첫번째 문단 가져오기\n",
    "    para = soup.select_one(\"p[dmcf-ptype='general']\")\n",
    "    print(f\"첫번째 문단 {para.text}\")\n",
    "\n",
    "    # 전체 본문 내용 가져오기\n",
    "    paras = soup.select(\"p[dmcf-ptype='general']\")\n",
    "    # print(paras)\n",
    "    for p in paras:\n",
    "        print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML>\n",
      "<html lang=\"ko\">\n",
      "<head>\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n",
      "    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,maximum-scale=1.0,minimum-scale=1.0,user-scalable=no\">\n",
      "    <meta name=\"description\" lang=\"ko\" content=\"잠시 후 다시 확인해주세요! : 네이버쇼핑\">\n",
      "    <title>에러 페이지 : 네이버쇼핑</title>\n",
      "    <link rel=\"stylesheet\" type=\"text/css\" href=\"//img.pay.naver.net/static/css/customer/naver_error.css\">\n",
      "\n",
      "    <script src=\"https://ssl.pstatic.net/static/fe/grafolio.js\"></script>\n",
      "</head>\n",
      "\n",
      "\n",
      "<body>\n",
      "<div id=\"u_skip\" class=\"u_skip\">\n",
      "    <a href=\"#content\">본문 바로가기</a>\n",
      "</div>\n",
      "<div class=\"wrap\">\n",
      "    <div class=\"header\" role=\"banner\">\n",
      "        <h1 class=\"logo\"><a href=\"//naver.com\" class=\"logo_link\"><img src=\"//img.pay.naver.net/static/images/customer/naver_logo.png\" width=\"90\" height=\"16\"\n",
      "                                                                             alt=\"네이버\"></a></h1>\n",
      "        <div class=\"nav\" role=\"navigation\">\n",
      "            <a href=\"//naver.com\" class=\"nav_link\">네이버홈</a>\n",
      "            <a href=\"//help.pay.naver.com\" class=\"nav_link\">쇼핑&페이 고객센터</a>\n",
      "        </div>\n",
      "    </div>\n",
      "    <hr>\n",
      "    <div class=\"container\" role=\"main\">\n",
      "        <div class=\"content\" id=\"content\">\n",
      "            <div class=\"image_area _errorImage\"></div>\n",
      "\n",
      "            <div class=\"info_area\">\n",
      "                <div class=\"info_txt\">\n",
      "                    <strong class=\"tit\">잠시 후 다시 확인해주세요!</strong>\n",
      "                    <p class=\"txt\">\n",
      "                        지금 이 서비스와 연결할 수 없습니다.<br>\n",
      "                        문제를 해결하기 위해 열심히 노력하고 있습니다.<br>\n",
      "                        잠시 후 다시 확인해주세요.\n",
      "                    </p>\n",
      "                </div>\n",
      "                <div class=\"info_link\">\n",
      "                    <a href=\"javascript:history.go(-1)\" class=\"link_prev\">이전 페이지</a><a href=\"//shopping.naver.com\" class=\"link_home\">네이버쇼핑 홈</a>\n",
      "                </div>\n",
      "            </div>\n",
      "        </div>\n",
      "    </div>\n",
      "    <hr>\n",
      "    <div class=\"footer\" role=\"contentinfo\">\n",
      "        <address>\n",
      "            <span>Copyright</span> ©<a href=\"http://www.navercorp.com\" class=\"link_naver\" target=\"_blank\">NAVER Corp.</a> <span>All Rights Reserved.</span>\n",
      "        </address>\n",
      "    </div>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "# url = \"https://shopping.naver.com/home\"\n",
    "url = \"https://shopping.naver.com/api/modules/gnb/category?id=root&_vc_=1717171131197\"\n",
    "\n",
    "userAgent = UserAgent()\n",
    "\n",
    "headers = {\"user-agent\":userAgent.chrome}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url, headers=headers)\n",
    "    print(r.text)\n",
    "    #soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보안뉴스더보기파이선 리포지터리에서 2년 간 잠자고 있던 패키지, 갑자기 업데이트 돼2월 26일\n",
      "연합뉴스더보기네이버, 파이선 무료 교육 참가할 차상위계층 고교생 모집1월 26일\n",
      "The Korea Daily더보기[알림] 'Python with DS 초급과정' 강좌 엽니다3월 7일\n",
      "ZUM 뉴스더보기\"AI 개발자 선호 1위 프로그래밍 언어는 '파이썬'…챗봇 개발 급증\"9일 전\n",
      "이투뉴스더보기한국코드페어, 'C언어, 파이선, 챗GPT 등' 온라인 SW강의 9종 무료공개2023. 9. 19.\n",
      "VISLA Magazine더보기DJ Python의 서울 데뷔 공연 @cakeshopseoul2023. 6. 15.\n",
      "헤럴드경제더보기방탄소년단 정국, 해외 차트 물들인 솔로앨범 ‘GOLDEN’…스포티파이선 전곡 줄세우기2023. 11. 5.서병기광고주: 서병기\n",
      "IT비즈뉴스더보기오라클자바교육학원, IT 취업준비생을 위한 '자바&파이선(AI) 풀스택' 무료교육 취업과정 모집2023. 9. 27.\n",
      "서울파이낸스더보기NH선물, 파이선 기반 '서핑보드API' 제공2023. 5. 25.\n",
      "한국경제더보기\"어디 파이선 할 줄 아는 사람 없나요?\"2019. 6. 25.\n",
      "교육연합신문더보기제주교육청, 예비고1 대상 '놀면 뭐하니? 파이선(Python)하자!(Level 2)' 코딩교육 운영2022. 1. 13.\n",
      "한겨레더보기파이선을 개발하다2021. 12. 30.\n",
      "보안뉴스더보기파이선 코드 공유 플랫폼인 PyPI에서 4천여 개 가짜 패키지 발견돼2021. 3. 8.\n",
      "연합뉴스더보기원티드랩 \"파이선 개발자 콘퍼런스에 공식 후원사 참여\"2022. 9. 22.\n",
      "보안뉴스더보기파이선 생태계에서 15년 간 아무도 고치지 않았던 취약점, 제2의 로그4셸 될까2022. 9. 22.\n",
      "보안뉴스더보기파이선 리포지터리에서 다시 나타난 정보 탈취 멀웨어2022. 12. 26.\n",
      "보안뉴스더보기페이스북, 파이선 코드 분석 도구인 파이사를 오픈소스로 전환2020. 8. 11.\n",
      "보안뉴스더보기파이선 개발자들 노리는 악성 패키지들, 암호화폐 노린다2022. 11. 9.\n",
      "한국경제더보기파이선 몰라도 AI 활용 가능하다고?2022. 11. 1.\n",
      "보안뉴스더보기보안 업체 이름 악용한 공급망 공격, 파이선 개발자들을 노렸다2022. 12. 20.\n",
      "한국경제더보기AI 파이선, 프로그래밍 언어 왕좌 넘본다2021. 4. 1.\n",
      "보안뉴스더보기인공지능 모델 공유 플랫폼 허깅페이스에서 위험한 취약점 발견돼4월 9일\n",
      "전자신문더보기[2022년 제16회 IT교육지원콘퍼런스]\"파이선 배우며 슬럼프 극복\"…박세진 학생(청란여고)2022. 11. 4.\n",
      "보안뉴스더보기또 PyPI! 정보 탈취 악성 코드 심겨진 패키지 10개 이상 발견돼2022. 8. 9.\n",
      "팍스경제TV더보기한국공인회계사회, '재무빅데이터분석사 자격시험' 국내 최초 신설…내년 1월 첫 시험2022. 10. 25.\n",
      "보안뉴스더보기새로운 파이선 기반 랜섬웨어, 주피터랩 웹 노트북 공격 중2022. 4. 1.\n",
      "조선일보더보기햄 브랜드 ‘스팸’ 왜 쏟아지는 광고 대명사가 됐나4월 8일\n",
      "전자신문더보기한국코드페어, 온라인 SW강의 파이선 등 6종 홈페이지 무료 공개2022. 9. 27.\n",
      "보안뉴스더보기[주말판] 현재 가장 인기 높은 데이터 과학 관련 도구와 기술 TOP 102023. 2. 4.\n",
      "보안뉴스더보기파이선 기반의 새로운 멀웨어, 후속 공격 위한 정보 수집 중2019. 4. 5.\n",
      "전자신문더보기이티에듀, '파이선·퓨전360' 원격 SW교육 본격화…온더라이브 기반2020. 11. 10.\n",
      "전자신문더보기[원격교육 시대, 무엇을 듣나]<1>파이썬 홈-딜리버리 스쿨, 전공 수업부터 실무 적용까지2021. 1. 6.\n",
      "보안뉴스더보기R? 파이선? SAS? 어떤 언어를 공부해야 할까?2018. 7. 25.\n",
      "교육정책뉴스더보기제주교육청, ‘놀면 뭐하니? 파이썬(Python)하자!’… 코딩 교육 과정 운영2022. 1. 13.\n",
      "보안뉴스더보기MS의 애저 클라우드 서비스에서 소스코드 유출시키는 낫레짓 취약점 나와2021. 12. 23.\n",
      "보안뉴스더보기개발자들의 필수 방문지인 공공 리포지터리가 위험하다2022. 5. 24.\n",
      "한국경제더보기주어진 데이터로 선박 수주량 예측하시오2023. 3. 7.\n",
      " 세계타임즈더보기인천교육과학연구원 파이선 활용 sw교육 연수 운영2018. 10. 29.\n",
      "보안뉴스더보기한국 10대는 해킹으로 시험지 빼돌리고, 유럽 10대는 리포지터리에 랜섬웨어 퍼트리고2022. 8. 4.\n",
      "토큰포스트더보기레이어2 네트워크 zkSync, 자바·Go·파이선 등 대중적 프로그래밍 언어 지원2022. 11. 2.\n",
      "요즘IT더보기파이썬 초보자가 저지르는 10가지 실수 | 요즘IT2022. 7. 26.\n",
      "서울파이낸스더보기한국공인회계사회, '재무빅데이터분석사 자격시험' 신설2022. 10. 25.\n",
      "케이벤치 (KBench)더보기디앤디, 게인워드 지포스 RTX 4060 파이썬 II 그래픽카드 출시3월 29일\n",
      "세계일보더보기[무료 명품강좌를 소개합니다] 컴퓨터 언어 알면… 왕초보도 프로그래밍 뚝딱2018. 7. 1.\n",
      "SBS 뉴스더보기[Pick] 종일 게임만 하던 아이…6살에 '최연소 프로그래머' 됐다2020. 11. 11.\n",
      "MBN더보기원티드랩 개발자가 비개발직군보다 연봉 평균 10% 더 받아2021. 10. 27.\n",
      "ITWorld Korea더보기\"왜 안 되는 걸까?\" 파이썬에서 기대할 수 없는 4가지 기능 개선2022. 5. 10.\n",
      "ZUM 뉴스더보기뱀에게도 귀가 있을까?…예상 뛰어넘는 청각의 비밀 [핵잼 사이언스]2023. 2. 18.\n",
      "서울경제신문더보기코딩에 빠진 기업은행2018. 10. 14.\n",
      "CIO Korea더보기파이썬 창시자가 말하는 사임 이유와 파이썬의 미래2018. 8. 6.\n"
     ]
    }
   ],
   "source": [
    "# 파이썬 뉴스 기사 크롤링\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.google.com/search?q=%ED%8C%8C%EC%9D%B4%EC%84%A0&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(\n",
    "        r.text, \"html.parser\"\n",
    "    )  # 가져온 데이터를  넣고 두번째 인자는 lxml 해당 객체로 파싱해달라는 얘기다\n",
    "\n",
    "    # 등장 입력 출력\n",
    "\n",
    "    #  뭘뤄줄지 몰라서 걍 한글 코딩\n",
    "\n",
    "    news = soup.find_all(\"article\", class_=\"IFHyqb DeXSAc\")\n",
    "\n",
    "    for new in news:\n",
    "        print(new.get_text())  # string 또는 get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
